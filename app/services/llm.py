import os
import time
from openai import AsyncOpenAI

# load_dotenv() - Removed

from app.db import get_active_model_full, get_llm_model_full_by_id
from app.core.logger import logger

async def analyze_text(text: str, prompt: str, llm_model_id: int = None):
    """
    Sends text and user prompt to LLM for processing.
    """
    db_config = None
    
    # 1. Try specific model if requested
    if llm_model_id:
        # Try new system first (Provider/Model)
        model_info = get_llm_model_full_by_id(llm_model_id)
        if model_info:
             db_config = {
                'api_key': model_info['api_key'],
                'base_url': model_info['base_url'],
                'model': model_info['model_name'],
                'api_type': model_info.get('api_type', 'chat_completions')
            }
        else:
             # Try legacy config
             from app.db import get_llm_config_by_id
             db_config = get_llm_config_by_id(llm_model_id)
    
    # 2. Try active model (New System)
    if not db_config:
        model_info = get_active_model_full()
        if model_info:
            db_config = {
                'api_key': model_info['api_key'],
                'base_url': model_info['base_url'],
                'model': model_info['model_name'],
                'api_type': model_info.get('api_type', 'chat_completions')
            }

    if not db_config:
        logger.error("âŒ No active LLM model configured in database.")
        return "âŒ è¯·å…ˆåœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®å¹¶æ¿€æ´»å¤§è¯­è¨€æ¨¡å‹(LLM)ã€‚", "unknown"
        
    api_key = db_config.get('api_key')
    base_url = db_config.get('base_url')
    model = db_config.get('model')
    api_type = db_config.get('api_type', 'chat_completions')

    logger.info(f"ğŸ¤– LLM Request -> Model: {model} | BaseURL: {base_url} | Protocol: {api_type}")

    if not api_key:
        return "âŒ LLM API Key is missing. è¯·åœ¨ç³»ç»Ÿè®¾ç½®ä¸­é…ç½®æœ‰æ•ˆçš„ API Keyã€‚", model
    
    client = AsyncOpenAI(api_key=api_key, base_url=base_url)

    sys_prompt = (
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹ç²¾ç‚¼ä¸“å®¶ï¼Œæ“…é•¿å¤„ç†å¤šæ–¹å¯¹è¯åŠå•äººæ¼”è®²çš„è¯­éŸ³è½¬æ–‡å­—(ASR)ææ–™ã€‚\n"
    "ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯ï¼š\n"
    "1. **è¯­å¢ƒçº é”™**ï¼šç»“åˆä¸Šä¸‹æ–‡çº æ­£åŒéŸ³é”™åˆ«å­—ã€‚\n"
    "2. **å™ªéŸ³å¤„ç†**ï¼šé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ä¿ç•™å£ç™–ï¼Œå¦åˆ™é»˜è®¤å‰”é™¤'å‘ƒ'ã€'é‚£ä¸ª'ã€'ç„¶å'ç­‰è¯­æ°”è¯ï¼Œç¼åˆç ´ç¢çš„å¥å­ã€‚\n"
    "3. **å¯¹è¯æ¢³ç†**ï¼šè‹¥æ–‡ä¸­å‡ºç°å¤šä¸ªå‘è¨€è€…ï¼Œè¯·è‡ªåŠ¨æ ¹æ®è¯­å¢ƒç†é¡ºé€»è¾‘å…³ç³»ï¼Œç¡®ä¿è¯­ä¹‰è¿è´¯ã€‚\n"
    "4. **ä¿æŒçœŸå®**ï¼šåœ¨æå‡å¯è¯»æ€§çš„åŒæ—¶ï¼Œä¸¥ç¦è™šæ„åŸå§‹æ–‡æœ¬ä¸­ä¸å­˜åœ¨çš„äº‹å®ã€‚\n"
    "5. **å¿ å®åº¦**ï¼šå¦‚æœç”¨æˆ·è¦æ±‚'é€å­—ç¨¿'æˆ–'ä¿ç•™å£ç™–'ï¼Œè¯·åŠ¡å¿…åŸæ ·ä¿ç•™æ‰€æœ‰è¯­æ°”è¯ï¼Œè¿™å¯¹äºå¿ƒç†åˆ†ææˆ–è¯­æ°”ç ”ç©¶è‡³å…³é‡è¦ã€‚\n"
    )
    user_content = f"""
[TASK_INSTRUCTIONS]
{prompt}

[RAW_TRANSCRIPT_START]
{text}
[RAW_TRANSCRIPT_END]
"""

    try:
        if api_type == 'responses':
            response = await client.responses.create(
                model=model,
                instructions=sys_prompt,
                input=user_content,
                temperature=0.7
            )
            return response.output_text, model
        else:
            # Default: chat_completions
            response = await client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": user_content}
                ],
                temperature=0.7
            )
            return response.choices[0].message.content, model
    except Exception as e:
        logger.error(f"âŒ LLM Error: {str(e)}")
        return f"âŒ LLM Error: {str(e)}", model


async def test_llm_connection(api_key: str, base_url: str, model: str, api_type: str = 'chat_completions'):
    """
    Test LLM provider connectivity with a minimal request.
    Returns { success: bool, message: str, latency_ms: int }
    """
    client = AsyncOpenAI(api_key=api_key, base_url=base_url)
    start = time.time()
    try:
        if api_type == 'responses':
            resp = await client.responses.create(
                model=model,
                input="Hi",
                max_output_tokens=5
            )
        else:
            resp = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "Hi"}],
                max_tokens=5
            )
        latency = round((time.time() - start) * 1000)
        return {"success": True, "message": "OK", "latency_ms": latency}
    except Exception as e:
        latency = round((time.time() - start) * 1000)
        return {"success": False, "message": str(e), "latency_ms": latency}
